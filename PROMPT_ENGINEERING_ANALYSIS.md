# Analyse du Prompt Engineering : query_all vs surgical

## üéØ Diff√©rences de prompts et leur impact sur la pr√©servation des r√©ponses

### 1Ô∏è‚É£ `grand_debat_query_all` - Prompt court et synth√©tique

**Fichier** : `server.py:1564-1578`

```python
prompt = f"""Tu es un analyste expert des contributions citoyennes du Grand D√©bat National 2019.
Analyse les donn√©es de {len(communes_loaded)} communes de Charente-Maritime et r√©ponds √† la question.

QUESTION: {query}

CONTEXTE DU GRAPHE DE CONNAISSANCES:
{context}

INSTRUCTIONS:
- Synth√©tise les informations de plusieurs communes
- Cite des exemples sp√©cifiques avec leurs communes d'origine
- Reste factuel et bas√© sur les donn√©es fournies
- R√©ponds en fran√ßais

R√âPONSE:"""

# Appel LLM
answer = await gpt_5_nano_complete(prompt, max_tokens=8192)
```

**Caract√©ristiques** :
- ‚ö†Ô∏è **"Synth√©tise"** : Encourage explicitement la compression
- ‚ö†Ô∏è **Pas de contrainte de longueur** : Limite implicite √† 8192 tokens
- ‚ö†Ô∏è **Contexte limit√©** : ~1,936 tokens (sans chunks de texte)
- ‚úÖ **En fran√ßais** : Adapt√© au contenu

**Impact sur pr√©servation** :
- üî¥ Le mot "synth√©tise" incite le LLM √† **r√©sumer** plut√¥t que pr√©server
- üî¥ Pas de guidance sur la structure attendue
- üî¥ Pas d'instruction explicite sur la longueur minimale

---

### 2Ô∏è‚É£ `grand_debat_surgical_query` - Prompt d√©taill√© et prescriptif

**Fichier** : `server.py:1813-1880` (pour surgical query)

```python
prompt = f"""Tu es un analyste expert des donn√©es citoyennes du Grand D√©bat National.
Ton r√¥le est de fournir une analyse EXHAUSTIVE et PR√âCISE bas√©e sur le graphe de connaissances reconstitu√©.

QUESTION: {query}

GRAPHE DE CONNAISSANCES RECONSTITU√â ({len(final_commune_ids)} communes sur {total_communes_available} disponibles,
expansion multi-hop 5 niveaux):
{context}

ONTOLOGIE CIVIQUE DU GRAPHE:
Entit√©s: PROPOSITION, THEMATIQUE, SERVICEPUBLIC, DOLEANCE, ACTEURINSTITUTIONNEL, OPINION, CITOYEN, CONCEPT,
REFORMEDEMOCRATIQUE, TERRITOIRE, COMMUNE, CONTRIBUTION
Relations: RELATED_TO, PROPOSE, FAIT_PARTIE_DE, CONCERNE, EXPRIME

[... suite du prompt ...]

R√àGLES ABSOLUES:
- Utilise OBLIGATOIREMENT les headers Markdown (# ##) pour structure forte
- Cite les entit√©s EXACTEMENT comme dans le contexte avec leur type ontologique
- Chaque fait DOIT avoir sa commune source et son entit√© justificative
- Si info non disponible: "Non document√© dans les {len(final_commune_ids)} communes analys√©es"
- R√©ponds en fran√ßais avec conviction chirurgicale

EXTRACTION CHIRURGICALE:"""

answer = await gpt_5_nano_complete(prompt, max_tokens=8192)
```

**Caract√©ristiques** :
- ‚úÖ **"EXHAUSTIVE et PR√âCISE"** : Encourage la compl√©tude
- ‚úÖ **R√®gles absolues** : Contraintes strictes sur citations et sources
- ‚úÖ **Structure impos√©e** : Headers Markdown obligatoires
- ‚úÖ **Contexte riche** : Inclut chunks de texte (~200k tokens potentiel)

---

### 3Ô∏è‚É£ `grand_debat_query_all_surgical` (via rag.aquery) - Prompt nano-graphrag

**Fichier** : `server.py:2050-2056` + `nano_graphrag/prompt.py:396-433`

```python
# Appel avec response_type personnalis√©
result = await rag.aquery(
    query,
    param=QueryParam(
        mode="local",
        return_provenance=include_sources,
        response_type="Comprehensive multi-commune analysis: 2500-5000 words total.
        Structure: Introduction (2-3 sentences) + ## Analyse par commune
        (one detailed paragraph per commune with provenance, 50-100 words each) +
        ## Synth√®se transversale (patterns and variations across communes)"
    )
)
```

**Prompt syst√®me nano-graphrag** :
```
---Role---

You are a helpful assistant responding to questions about data in the tables provided.

---Goal---

Generate a response of the target length and format that responds to the user's question,
summarizing all information in the input data tables appropriate for the response length
and format, and incorporating any relevant general knowledge.

If you don't know the answer, just say so. Do not make anything up.
Do not include information where the supporting evidence for it is not provided.

---Target response length and format---

{response_type}  ‚Üê "Comprehensive multi-commune analysis: 2500-5000 words..."

---Data tables---

{context_data}  ‚Üê Inclut entities, relationships, communities, ET chunks de texte

---Goal---

Generate a response of the target length and format...
If you don't know the answer, just say so. Do not make anything up.
Do not include information where the supporting evidence for it is not provided.

---Target response length and format---

{response_type}

Add sections and commentary to the response as appropriate for the length and format.
Style the response in markdown.
```

**Caract√©ristiques** :
- ‚úÖ **Longueur explicite** : "2500-5000 words total"
- ‚úÖ **Structure d√©taill√©e** : Introduction + Analyse par commune + Synth√®se
- ‚úÖ **Provenance obligatoire** : "with provenance" dans response_type
- ‚úÖ **R√©p√©t√© deux fois** : Le goal et response_type sont r√©p√©t√©s pour insister
- ‚úÖ **"Do not make anything up"** : Limite les hallucinations
- ‚úÖ **Contexte complet** : Data tables incluent chunks de texte

---

## üìä Comparaison des prompts

| Aspect | `query_all` | `surgical query` | `surgical via rag.aquery` |
|--------|-------------|------------------|---------------------------|
| **Mot-cl√© principal** | "Synth√©tise" ‚ùå | "EXHAUSTIVE" ‚úÖ | "Comprehensive" ‚úÖ |
| **Longueur guid√©e** | Non ‚ùå | Non | Oui "2500-5000 words" ‚úÖ |
| **Structure impos√©e** | Non ‚ùå | Oui (headers Markdown) ‚úÖ | Oui (d√©taill√©e) ‚úÖ |
| **Citations obligatoires** | "Cite des exemples" ‚ö†Ô∏è | "DOIT avoir source" ‚úÖ | "with provenance" ‚úÖ |
| **Protection hallucinations** | "Reste factuel" ‚ö†Ô∏è | Oui ‚úÖ | "Do not make anything up" ‚úÖ |
| **Contexte disponible** | 1,936 tokens ‚ùå | ~10k tokens | ~200k tokens ‚úÖ |
| **Chunks de texte** | Non ‚ùå | Oui ‚úÖ | Oui ‚úÖ |

---

## üîç Impact du mot "Synth√©tise"

### Prompt `query_all`
```
INSTRUCTIONS:
- Synth√©tise les informations de plusieurs communes  ‚Üê COMPRESSION ENCOURAG√âE
```

**Effet psychologique sur le LLM** :
- "Synth√©tiser" = **r√©sumer**, **condenser**, **r√©duire**
- Le LLM va naturellement **supprimer les d√©tails** pour faire tenir tout dans une r√©ponse concise
- Contradictoire avec "Cite des exemples sp√©cifiques"

### Prompt `surgical query`
```
Ton r√¥le est de fournir une analyse EXHAUSTIVE et PR√âCISE  ‚Üê COMPL√âTUDE ENCOURAG√âE
```

**Effet psychologique sur le LLM** :
- "EXHAUSTIVE" = **tout inclure**, **ne rien omettre**
- "PR√âCISE" = **d√©tails exacts**, **citations textuelles**
- Renforce la pr√©servation de l'information

### Prompt `rag.aquery`
```
Target response length: 2500-5000 words total  ‚Üê LONGUEUR MINIMALE IMPOS√âE
Structure: ... (one detailed paragraph per commune with provenance, 50-100 words each)
```

**Effet psychologique sur le LLM** :
- Longueur minimale **force** le LLM √† d√©velopper
- Structure d√©taill√©e **garantit** qu'aucune commune n'est oubli√©e
- "with provenance" **oblige** les citations

---

## üí° Recommandations de prompt engineering

### Pour am√©liorer `grand_debat_query_all`

Si vous voulez garder l'architecture √† un seul LLM call, modifiez le prompt :

```python
prompt = f"""Tu es un analyste expert des contributions citoyennes du Grand D√©bat National 2019.
Ton r√¥le est de fournir une ANALYSE EXHAUSTIVE ET D√âTAILL√âE des {len(communes_loaded)} communes de Charente-Maritime.

QUESTION: {query}

CONTEXTE DU GRAPHE DE CONNAISSANCES:
{context}

INSTRUCTIONS IMP√âRATIVES:
- Produis une r√©ponse de 3000-5000 mots minimum
- Structure OBLIGATOIRE:
  * Introduction (2-3 phrases)
  * ## Analyse d√©taill√©e par commune (un paragraphe de 100-150 mots PAR commune)
  * ## Synth√®se transversale (patterns communs et variations)
- Chaque fait DOIT inclure:
  * Citation textuelle entre guillemets
  * Source: [Nom commune]
  * Type d'entit√© si disponible
- NE R√âSUME PAS : Pr√©serve tous les d√©tails importants
- Si information manquante: indique clairement "Non document√©"
- R√©ponds en fran√ßais avec pr√©cision chirurgicale

ANALYSE EXHAUSTIVE:"""
```

**Changements cl√©s** :
1. ‚ùå **Retire** "Synth√©tise"
2. ‚úÖ **Ajoute** "EXHAUSTIVE ET D√âTAILL√âE"
3. ‚úÖ **Impose** longueur minimale "3000-5000 mots"
4. ‚úÖ **Structure** obligatoire par commune
5. ‚úÖ **"NE R√âSUME PAS"** : Instruction explicite anti-compression
6. ‚úÖ **Citations obligatoires**

### Mais le vrai probl√®me reste...

**M√™me avec un meilleur prompt**, `query_all` souffre de :
- ‚ùå **Pas de chunks de texte** : Le contexte n'a que des noms d'entit√©s
- ‚ùå **Contexte limit√©** : ~1,936 tokens vs ~200k dans surgical
- ‚ùå **Un seul appel LLM** : Doit tout compresser dans 8192 tokens

**Le prompt ne peut pas compenser l'absence de donn√©es sources !**

---

## üéØ Conclusion sur le prompt engineering

| Probl√®me | Impact | Solution |
|----------|--------|----------|
| Mot "Synth√©tise" | Encourage compression | ‚ùå Changer en "EXHAUSTIVE" |
| Pas de longueur min | LLM peut √™tre bref | ‚ùå Imposer "3000-5000 mots" |
| Pas de structure | Communes oubli√©es | ‚ùå Structure par commune |
| **Pas de chunks** üî¥ | **Donn√©es manquantes** | ‚úÖ **Utiliser surgical** |

**Le prompt engineering peut am√©liorer marginalement, mais ne r√©sout pas le probl√®me fondamental : l'absence de chunks de texte dans query_all.**

---

## üìà Tests propos√©s

### Test A : Modifier uniquement le prompt de query_all
Sans changer l'architecture, am√©liorer le prompt pour voir si √ßa aide.

**Attendu** : L√©g√®re am√©lioration, mais toujours <20% de pr√©servation car contexte limit√©.

### Test B : Comparer surgical avec diff√©rents response_type
Tester l'impact de la longueur guid√©e :

```python
# Version courte
response_type="Brief summary: 500 words"

# Version longue
response_type="Comprehensive analysis: 5000 words"
```

**Attendu** : La version longue pr√©serve plus d'information.

### Test C : Mesurer l'impact de "Synth√©tise" vs "EXHAUSTIVE"
Deux versions de query_all identiques sauf le mot-cl√©.

**Attendu** : "EXHAUSTIVE" produit des r√©ponses ~30% plus longues.

---

## üî¨ M√©thodologie d'√©valuation

Pour mesurer l'impact du prompt engineering :

1. **Compter les citations** : Nombre de citations textuelles entre guillemets
2. **Longueur de r√©ponse** : Nombre de mots
3. **Communes mentionn√©es** : Nombre de communes cit√©es explicitement
4. **D√©tails pr√©serv√©s** : Pr√©sence de chiffres, noms sp√©cifiques, dates

**Exemple de m√©trique** :
```python
def evaluate_response(response):
    return {
        'word_count': len(response.split()),
        'citation_count': response.count('"'),
        'commune_mentions': len(set(re.findall(r'\[([\w\s]+)\]', response))),
        'specificity_score': len(re.findall(r'\d+|[A-Z][a-z√©]+\s[A-Z][a-z√©]+', response))
    }
```

---

## ‚úÖ Recommandation finale

**Le prompt engineering seul ne suffit pas.**

Pour une pr√©servation optimale des r√©ponses :
1. ‚úÖ Utilisez `grand_debat_query_all_surgical` (chunks + bon prompt)
2. ‚ö†Ô∏è Si vous devez utiliser `query_all`, am√©liorez le prompt ET ajoutez la r√©cup√©ration des chunks
3. ‚ùå Ne comptez pas uniquement sur le prompt pour compenser l'absence de donn√©es

**La combinaison gagnante** : `Chunks de texte + Prompt exhaustif + Longueur guid√©e`
