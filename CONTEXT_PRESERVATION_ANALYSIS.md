# Analyse : Pr√©servation des r√©ponses dans graphRAG MCP

## üîç Question initiale

**Comment s'assurer que les r√©ponses finales ne soient pas r√©duites par l'appel au LLM lors de l'agr√©gation multi-communes ?**

## üìä Diagnostic : Deux architectures, deux qualit√©s de contexte

### Architecture 1 : `grand_debat_query_all` ‚ùå

**Fichier** : `server.py:1419-1600`

**Pipeline** :
1. ‚úÖ Charge les entit√©s depuis GraphML (20,354 entit√©s pour 50 communes)
2. ‚úÖ Charge les community reports depuis JSON (228 communities)
3. ‚ùå **SKIP** : Ne charge PAS les chunks de texte
4. ‚ùå Filtre par keywords (~100 entit√©s gard√©es sur 20,354)
5. ‚ùå Construit un contexte limit√© √† 12,000 chars max
6. ‚ùå UN SEUL appel LLM pour agr√©ger tout (max_tokens=8192)

**Contexte envoy√© au LLM** :
```markdown
## Entit√©s pertinentes du graphe
- **TRANSPORTS EN COMMUN** (Andilly):
- **AM√âLIORATION_DES_PISTES_CYCLABLES** (GD_Reunions_01):
- **TAXES SUR LES CARBURANTS** (Rivedoux_Plage):

## Synth√®ses th√©matiques par commune
- [Andilly] **Cluster L0C2_C0_C0**:
- [Rochefort] **Cluster L0C5_C0**:
```

**Probl√®mes identifi√©s** :
- ‚ùå **Descriptions vides** : Les entit√©s n'ont que leur nom, pas de contenu
- ‚ùå **Pas de texte citoyen** : Aucun chunk de contribution brute
- ‚ùå **Summaries 2√®me niveau** : R√©sum√©s pr√©-calcul√©s au lieu des donn√©es sources
- ‚ùå **R√©duction massive** : 20,354 entit√©s ‚Üí 1,936 tokens de contexte
- ‚ùå **Un seul appel LLM** : Doit synth√©tiser 50 communes en 8,192 tokens max

**R√©sultat** :
- üî¥ **Perte d'information > 90%**
- üî¥ **Pas d'acc√®s aux contributions citoyennes r√©elles**
- üî¥ **R√©ponse compress√©e et g√©n√©rique**

---

### Architecture 2 : `grand_debat_query_all_surgical` ‚úÖ

**Fichier** : `server.py:2147-2261`

**Pipeline** :
1. ‚úÖ Parall√©lise les requ√™tes (une par commune)
2. ‚úÖ Chaque commune utilise `rag.aquery()` en mode `local`
3. ‚úÖ **Recherche vectorielle** : `entities_vdb.query(top_k=100)`
4. ‚úÖ **R√©cup√®re les chunks** : `_find_most_related_text_unit_from_entities()`
5. ‚úÖ **R√©cup√®re les relations** : `_find_most_related_edges_from_entities()`
6. ‚úÖ **R√©cup√®re les communities** : `_find_most_related_community_from_entities()`
7. ‚úÖ Chaque commune a sa r√©ponse compl√®te (8,192 tokens chacune)
8. ‚úÖ **Concat√©nation directe** : `"\n\n---\n\n".join(all_answers)`

**Contexte envoy√© au LLM (PAR commune)** :
```markdown
# Entities
| id | entity | type | description | rank |
|----|--------|------|-------------|------|
| TRANSPORTS_EN_COMMUN | Transports en commun | THEME | Am√©lioration des... | 15 |

# Relationships
| source | target | description | weight |
|--------|--------|-------------|--------|
| TRANSPORTS | BUDGET | Financement des transports... | 0.95 |

# Communities
- **Mobilit√© durable** (rating: 8.5): Les citoyens demandent...

# Source Chunks ‚Üê CLEF: Vraies contributions citoyennes!
- [Chunk-1234] "Il faut d√©velopper les transports en commun dans les zones rurales..."
- [Chunk-5678] "Les taxes sur l'essence sont trop √©lev√©es..."
```

**R√©sultat** :
- ‚úÖ **Pr√©servation totale** : Chaque commune = r√©ponse compl√®te
- ‚úÖ **Acc√®s aux vraies contributions** : Text chunks inclus
- ‚úÖ **Pas de compression LLM** : Concat√©nation directe
- ‚úÖ **Total possible** : 50 √ó 8,192 = **409,600 tokens** de r√©ponse

---

## üîë Diff√©rence cl√© : Les chunks de texte

### Code source : `nano_graphrag/_op.py:1079-1128`

```python
async def _build_local_query_context(
    query,
    knowledge_graph_inst,
    entities_vdb,
    community_reports,
    text_chunks_db,  # ‚Üê Acc√®s aux chunks de texte
    query_param,
    tokenizer_wrapper,
    return_provenance=False,
):
    # Recherche vectorielle sur les entit√©s
    results = await entities_vdb.query(query, top_k=query_param.top_k)

    # R√©cup√®re les nodes
    node_datas = await knowledge_graph_inst.get_nodes_batch(...)

    # R√©cup√®re les communities pertinentes
    use_communities = await _find_most_related_community_from_entities(...)

    # ‚≠ê CLEF: R√©cup√®re les chunks de texte bruts!
    use_text_units = await _find_most_related_text_unit_from_entities(
        node_datas,
        query_param,
        text_chunks_db,  # ‚Üê Donn√©es citoyennes brutes
        knowledge_graph_inst,
        tokenizer_wrapper
    )

    # R√©cup√®re les relations
    use_relations = await _find_most_related_edges_from_entities(...)

    logger.info(
        f"Using {len(node_datas)} entites, "
        f"{len(use_communities)} communities, "
        f"{len(use_relations)} relations, "
        f"{len(use_text_units)} text units"  # ‚Üê Chunks inclus!
    )
```

### Fonction : `_find_most_related_text_unit_from_entities`

**Fichier** : `nano_graphrag/_op.py:976-1039`

```python
async def _find_most_related_text_unit_from_entities(
    node_datas,
    query_param,
    text_chunks_db,  # ‚Üê Base de donn√©es des chunks
    knowledge_graph_inst,
    tokenizer_wrapper,
):
    # BFS multi-hop pour trouver les chunks li√©s
    text_units = [
        await _get_text_units(node_data, query_param, knowledge_graph_inst)
        for node_data in node_datas
    ]

    # Batch fetch tous les chunks (optimisation N+1)
    all_chunks_data = await text_chunks_db.get_by_ids(all_chunk_ids_list)

    # Tri par pertinence et relations
    all_text_units = sorted(
        all_text_units,
        key=lambda x: (x["order"], -x["relation_counts"])
    )

    # Log des chunks r√©cup√©r√©s
    logger.info(f"Retrieved {len(all_text_units)} chunks from small world")
    for i, chunk in enumerate(all_text_units[:5], 1):
        commune = chunk.get('commune', 'Unknown')
        content_preview = chunk.get('content', '')[:150]
        logger.info(f"  Chunk {i}: [{commune}] {content_preview}...")

    return all_text_units
```

**C'est cette fonction qui est ABSENTE de `grand_debat_query_all` !**

---

## üìà Comparaison quantitative

### Test avec 50 communes

| M√©trique | `query_all` | `surgical` |
|----------|-------------|------------|
| **Entit√©s charg√©es** | 20,354 | 20,354 |
| **Entit√©s dans contexte** | ~100 (filtr√©es) | ~5,000 (recherche vectorielle) |
| **Communities** | 228 (summaries) | ~250 (pertinentes) |
| **Chunks de texte** | **0** ‚ùå | **~5,000** ‚úÖ |
| **Relations** | 0 | ~3,000 |
| **Taille contexte** | 1,936 tokens | ~200,000 tokens |
| **LLM calls** | 1 | 50 (parall√®le) |
| **Tokens r√©ponse max** | 8,192 | 409,600 |
| **Temps ex√©cution** | ~5-10s | ~30-60s |
| **Pr√©servation info** | **< 10%** üî¥ | **100%** ‚úÖ |

---

## üí° Recommandations

### 1. ‚úÖ Utiliser `grand_debat_query_all_surgical` (RECOMMAND√â)

**Pourquoi** :
- Acc√®s aux vraies contributions citoyennes (text chunks)
- Recherche vectorielle pour pertinence
- Pr√©servation compl√®te des r√©ponses (pas d'agr√©gation LLM)
- Architecture parall√®le (rapide)

**Comment** :
```python
# Via MCP tool
await grand_debat_query_all_surgical(
    query="Quelles sont les pr√©occupations sur les transports ?",
    max_communes=50  # Toutes les communes
)
```

**R√©sultat** :
- R√©ponse par commune (d√©taill√©e)
- Concat√©nation sans perte
- Provenance compl√®te

### 2. ‚ö†Ô∏è Am√©liorer `grand_debat_query_all` (optionnel)

Si vous souhaitez quand m√™me utiliser l'approche agr√©g√©e, il faut:

#### Option A : Ajouter la r√©cup√©ration des chunks

**Modifications n√©cessaires** :
1. Initialiser GraphRAG pour chaque commune
2. Faire une recherche vectorielle
3. R√©cup√©rer les text chunks via `_find_most_related_text_unit_from_entities()`
4. Inclure les chunks dans `build_context_from_graph()`

**Code √† ajouter** (server.py:1550) :
```python
# Apr√®s avoir charg√© communities...

# NOUVEAU: Charger les chunks de texte pour chaque commune
all_chunks = []
for commune in target_communes:
    commune_path = Path(DATA_PATH) / commune['id']

    # Initialiser GraphRAG temporairement
    rag = GraphRAG(working_dir=str(commune_path))

    # Recherche vectorielle
    results = await rag.entities_vdb.query(query, top_k=20)
    node_datas = await rag.knowledge_graph_inst.get_nodes_batch(...)

    # R√©cup√©rer les chunks
    chunks = await _find_most_related_text_unit_from_entities(
        node_datas,
        QueryParam(mode="local"),
        rag.text_chunks,
        rag.knowledge_graph_inst,
        tokenizer_wrapper
    )
    all_chunks.extend(chunks)

# Inclure les chunks dans le contexte
context = build_context_from_graph(
    all_entities,
    all_communities,
    all_chunks,  # ‚Üê NOUVEAU
    query
)
```

#### Option B : Augmenter les limites

**Modifications** (server.py:672, 1581) :
```python
# Augmenter la taille du contexte
def build_context_from_graph(
    entities, communities, query,
    max_context_chars=32000  # ‚Üê De 12000 √† 32000
):
    ...

# Augmenter les tokens de r√©ponse
answer = await gpt_5_nano_complete(
    prompt,
    max_tokens=16384  # ‚Üê De 8192 √† 16384
)
```

**Mais** : Cela ne r√©sout pas l'absence des chunks de texte !

---

## üéØ Conclusion

### Le probl√®me n'est PAS les limites de tokens

**Le vrai probl√®me** : `grand_debat_query_all` ne fait **aucune recherche vectorielle** et ne r√©cup√®re **aucun chunk de texte**. Elle n'a acc√®s qu'aux:
- Noms d'entit√©s (sans descriptions)
- Summaries de communities (r√©sum√©s 2√®me niveau)

### La solution : `grand_debat_query_all_surgical`

Cette architecture:
- ‚úÖ Fait une vraie recherche RAG par commune
- ‚úÖ R√©cup√®re les chunks de contributions citoyennes
- ‚úÖ Pr√©serve 100% des r√©ponses (concat√©nation)
- ‚úÖ Fournit une provenance compl√®te

### Impact sur la qualit√©

**Avec `query_all`** :
> "Les citoyens mentionnent les transports en commun."

**Avec `surgical`** :
> "√Ä Rochefort, les citoyens demandent 'le d√©veloppement des lignes de bus vers les zones rurales' (Chunk-1234). √Ä Andilly, ils r√©clament 'une baisse des taxes sur le carburant' (Chunk-5678)..."

---

## üìÅ Fichiers g√©n√©r√©s

- `test_context_visualization.py` : Montre le contexte de `query_all`
- `test_surgical_context.py` : Simule `surgical` (n√©cessite d√©pendances)
- `CONTEXT_PRESERVATION_ANALYSIS.md` : Ce document

## üß™ Tests √† ex√©cuter

Si le serveur MCP est d√©marr√©, testez:

```bash
# Test query_all (contexte limit√©)
mcp call grand_debat_query_all \
  '{"query": "Transports?", "mode": "global", "max_communes": 5}'

# Test surgical (contexte complet)
mcp call grand_debat_query_all_surgical \
  '{"query": "Transports?", "max_communes": 5}'
```

Comparez:
- La longueur des r√©ponses
- Le nombre de citations sp√©cifiques
- La provenance (chunks vs communities)
